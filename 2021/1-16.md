
# 2021-1-16

### error read: https://rsshub.app/gov/ndrc/xwdt

### error read: http://www.bigdatainterview.com/feed/

### [机器学习(一)：5分钟理解机器学习并上手实践](https://segmentfault.com/a/1190000038997134)

### [读懂“十四五”新发展格局下的改革议程](http://www.jintiankansha.me/t/Pn8nFWI8Fy)

### [该奋斗的年龄，不要选择了安逸！](http://www.jintiankansha.me/t/C4Q84AVGKW)

### [不平凡的2020：重仓持有金融地产，从年初锤到年尾](http://www.jintiankansha.me/t/3mnbhJCPGj)

### [太刺激了！这只4000亿抱团股一度跌停！午后快速拉升大涨7%！过去半年股价曾翻5倍！发生了什么？](http://www.jintiankansha.me/t/XNifXsgRXz)

### [给初级投资者的18条建议](http://www.jintiankansha.me/t/H2RzfSDAtX)

### [上市首日股价涨超90%！稻草熊的成长性几何？](http://www.jintiankansha.me/t/SmICPK3VYg)

### [春运期间健康码全国互认](https://m.21jingji.com/article/20210115/herald/b0fc29b50a4846d0e3fd18cb64a95685.html)

### [Comic for 2021.01.15](http://www.explosm.net/comics/5768/)

### [Happy birthday，20岁的维基百科！](http://jandan.net/p/108333)

### [大吐槽：”煎蛋网正在争夺每日一猫和每日一兔的优先权“](http://jandan.net/p/108355)

### [美国每年有近1亿只老鼠为科学献身？](http://jandan.net/p/108347)

### [学习 Python 的好文章](https://linux.cn/article-13018-1.html?utm_source=rss&utm_medium=rss)

### [PipeCAD Project](http://www.cppblog.com/eryar/archive/2021/01/15/pipecad_project.html)

### [JAVASCRIPT FUNCTIONS 详解](https://www.infoq.cn/article/plOA1ttH4RQWVLqF9xCc)

### [JAVASCRIPT OBJECTS](https://www.infoq.cn/article/VR2kPl6k4LNmR2Wkexr5)

### [国内酒店稳定性治理实践之内部资源治理](https://www.infoq.cn/article/qqdsxPipJ83OCBgMPfMj)

### [腾讯课堂百万师生同时在线，如何实现消息的稳定互动？](https://www.infoq.cn/article/FxGp5OLPApuVbaiJDfo8)

### [【Free Data Science Foundation Bootcamp 】网页链接 免费数据科学基础训练营。数据科学毫无疑问是一个光明而有前途的职业选择，那咱们来一起训练吧！ 网路冷眼...](https://weibo.com/1715118170/JDlTdolsd)

### [#绿洲摄影#中国.云南.大理.崇圣寺三塔 绿洲 [图片][图片][图片][图片]](https://weibo.com/1715118170/JDlOcA6Yq)

### [【Sum and average of array elements in C++】网页链接 C ++中数组元素的总和和平均值。 [图片]](https://weibo.com/1715118170/JDlHbxRG9)

### [【Using GPT-3 for plain language incident root cause from logs】网页链接 使用GPT-3记录日志中的纯语言事件根本原因。网路冷眼技术分享 #科技暖心季# [图片]...](https://weibo.com/1715118170/JDluRtLSj)

### [【A step-by-step guide for semantic functional code】网页链接 语义函数式代码分步指南。 [图片]](https://weibo.com/1715118170/JDliYwkiw)

### [【Amplication – Instantly Generate Node.js Apps with GraphQL and REST API】网页链接 Amplication – 使用GraphQL和REST API即时生成Node.js应用。  网路冷...](https://weibo.com/1715118170/JDl6vtGcN)

### [//@_阿楠_://@纪录片之家:转发微博 - 转发 @局外人看电影:&ensp;徐皓峰：“在这个如梦如戏的人间，做一件自己喜欢的事儿吧。” [图片][图片][图片][图片][图片][...](https://weibo.com/1642628345/JDlnpAgw1)

### [ToTTo: A Controlled Table-to-Text Generation Dataset](http://feedproxy.google.com/~r/blogspot/gJZg/~3/jeMkmAfQxOk/totto-controlled-table-to-text.html)

 <span class="byline-author">Posted by Ankur Parikh and Xuezhi Wang, Research Scientists, Google Research</span> <p>In the last few years, research in <a href="https://en.wikipedia.org/wiki/Natural-language_generation">natural language generation</a>, used for tasks like text summarization, has made tremendous progress. Yet, despite achieving high levels of fluency, neural systems can still be prone to <a href="https://arxiv.org/abs/1707.08052">hallucination</a> (i.e.generating text that is understandable, but not faithful to the source), which can prohibit these systems from being used in many applications that require high degrees of accuracy. Consider an example from the <a href="https://arxiv.org/abs/1603.07771">Wikibio dataset</a>, where the neural <a href="https://arxiv.org/abs/1704.04368">baseline model</a> tasked with summarizing a Wikipedia infobox entry for Belgian football player <a href="https://en.wikipedia.org/wiki/Constant_Vanden_Stock">Constant Vanden Stock</a> summarizes incorrectly that he is an American figure skater. </p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-jrVMikqK0fo/YAHTkDqSt5I/AAAAAAAAHBA/jNegsPhq6tkOcRsUyGsSivo6LnAKYuL_wCLcBGAsYHQ/s623/image3%2B%25283%2529.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="216" src="https://1.bp.blogspot.com/-jrVMikqK0fo/YAHTkDqSt5I/AAAAAAAAHBA/jNegsPhq6tkOcRsUyGsSivo6LnAKYuL_wCLcBGAsYHQ/w640-h216/image3%2B%25283%2529.jpg" width="640" /></a></div><p>While the process of assessing the faithfulness of generated text to the source content can be challenging, it is often easier when the source content is structured (e.g., in tabular format). Moreover, structured data can also test a model’s ability for reasoning and numerical inference. However, existing large scale structured datasets are often noisy (i.e., the reference sentence cannot be fully inferred from the tabular data), making them unreliable for the measurement of hallucination in model development.</p><p>In “<a href="https://arxiv.org/abs/2004.14373">ToTTo: A Controlled Table-To-Text Generation Dataset</a>”,  we present an open domain table-to-text generation dataset created using a novel annotation process (via sentence revision) along with a controlled text generation task that can be used to assess model hallucination. ToTTo (shorthand for “Table-To-Text”) consists of 121,000 training examples, along with 7,500 examples each for development and test. Due to the accuracy of annotations, this dataset is suitable as a challenging benchmark for research in high precision text generation. The dataset and code are open-sourced on <a href="https://github.com/google-research-datasets/totto">our GitHub repo</a>.  </p><p><b>Table-to-Text Generation</b><br />ToTTo introduces a <em>controlled </em>generation task in which a given Wikipedia table with a set of selected cells is used as the source material for the task of producing a single sentence description that summarizes the cell contents in the context of the table. The example below demonstrates some of the many challenges posed by the task, such as numerical reasoning, a large open-domain vocabulary, and varied table structure. </p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-MNhjVvcI6kg/YAHT5Y7Q4jI/AAAAAAAAHBM/eSd9jZSEFwcSUW9iWee8TdICIQTlAHLKQCLcBGAsYHQ/s624/image1%2B%25285%2529.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" height="306" src="https://1.bp.blogspot.com/-MNhjVvcI6kg/YAHT5Y7Q4jI/AAAAAAAAHBM/eSd9jZSEFwcSUW9iWee8TdICIQTlAHLKQCLcBGAsYHQ/w640-h306/image1%2B%25285%2529.jpg" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Example in the ToTTo dataset, where given the source table and set of highlighted cells (<b>left</b>), the goal is to generate a one sentence description, such as the “target sentence” (<b>right</b>). Note that generating the target sentence would require numerical inference (eleven NFL seasons) and understanding of the NFL domain.</td></tr></tbody></table><p><b>Annotation Process</b><br />Designing an annotation process to obtain natural but also clean target sentences from tabular data  is a significant challenge. Many datasets like <a href="https://arxiv.org/abs/1603.07771">Wikibio</a> and <a href="https://arxiv.org/abs/1707.08052">RotoWire</a> pair naturally occurring text heuristically with tables, a noisy process that makes it difficult to disentangle whether hallucination is primarily caused by data noise or model shortcomings. On the other hand, one can elicit annotators to write sentence targets <a href="https://www.aclweb.org/anthology/W17-3518.pdf">from scratch</a>, which are faithful to the table, but the resulting targets often <a href="https://arxiv.org/pdf/1803.02324.pdf">lack variety</a> in terms of structure and style. </p><p>In contrast, ToTTo is constructed using a novel data annotation strategy in which annotators revise existing Wikipedia sentences in stages. This results in target sentences that are clean, as well as natural, containing interesting and varied linguistic properties. The data collection and annotation process begins by collecting tables from Wikipedia, where a given table is paired with a summary sentence collected from the supporting page context according to heuristics, such as word overlap between the page text and the table and hyperlinks referencing tabular data. This summary sentence may contain information not supported by the table and may contain pronouns with antecedents found in the table only, not the sentence itself.  </p><p>The annotator then highlights the cells in the table that support the sentence and deletes phrases in the sentence that are not supported by the table. They also decontextualize the sentence so that it is standalone (e.g., with correct pronoun resolution) and correct grammar, where necessary. </p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-cLmDPneHYnU/YAHUEKzCirI/AAAAAAAAHBQ/SAM8Qj1ra5E3y5pzKKhsDLMkf0wDuXqogCLcBGAsYHQ/s612/image4%2B%25282%2529.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="328" src="https://1.bp.blogspot.com/-cLmDPneHYnU/YAHUEKzCirI/AAAAAAAAHBQ/SAM8Qj1ra5E3y5pzKKhsDLMkf0wDuXqogCLcBGAsYHQ/w640-h328/image4%2B%25282%2529.jpg" width="640" /></a></div><p>We show that annotators obtain high agreement on the above task: 0.856 <a href="https://en.wikipedia.org/wiki/Fleiss%27_kappa">Fleiss Kappa</a> for cell highlighting, and 67.0 <a href="https://www.aclweb.org/anthology/P02-1040/">BLEU</a> for the final target sentence.</p><p><b>Dataset Analysis</b><br />We conducted a topic analysis on the ToTTo dataset over 44 categories and found that the Sports and Countries topics, each of which consists of a range of fine-grained topics, e.g., football/olympics for sports and population/buildings for countries, together comprise 56.4% of the dataset. The other 44% is composed of a much more broad set of topics, including Performing Arts, Transportation, and Entertainment.  </p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-9OnRFuDlei4/YAHUMryhVOI/AAAAAAAAHBY/57MQpLcPpU0d678I4XhLeU9KjOXInTntACLcBGAsYHQ/s361/image2.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="214" src="https://1.bp.blogspot.com/-9OnRFuDlei4/YAHUMryhVOI/AAAAAAAAHBY/57MQpLcPpU0d678I4XhLeU9KjOXInTntACLcBGAsYHQ/w400-h214/image2.png" width="400" /></a></div><p>Furthermore, we conducted a manual analysis of the different types of linguistic phenomena in the dataset over 100 randomly chosen examples. The table below summarizes the fraction of examples that require reference to the page and section titles, as well as some of the linguistic phenomena in the dataset that potentially pose new challenges to current systems. </p><table align="center" cellpadding="0" cellspacing="5" style="margin-left: auto; margin-right: auto;">  <tbody><tr>   <td><b>Linguistic Phenomena</b>   </td>   <td align="center"><b>Percentage</b>   </td>  </tr>  <tr>   <td>Require reference to page title    </td>   <td align="center">82%    </td>  </tr>  <tr>   <td>Require reference to section title    </td>   <td align="center">19%    </td>  </tr>  <tr>   <td>Require reference to table description    </td>   <td align="center">3%    </td>  </tr>  <tr>   <td>Reasoning (logical, numerical, temporal etc.)    </td>   <td align="center">21%    </td>  </tr>  <tr>   <td>Comparison across rows/columns/cells    </td>   <td align="center">13%    </td>  </tr>  <tr>   <td>Require background information    </td>   <td align="center">12%    </td>  </tr></tbody></table><p><b>Baseline Results</b><br />We present some baseline results of three state-of-the-art models from the literature (<a href="https://arxiv.org/abs/1907.12461">BERT-to-BERT</a>, <a href="https://arxiv.org/abs/1704.04368">Pointer Generator</a>, and the <a href="https://arxiv.org/abs/1809.00582">Puduppully 2019 model</a>) on two evaluation metrics, <a href="https://www.aclweb.org/anthology/P02-1040/">BLEU</a> and <a href="https://arxiv.org/abs/1906.01081">PARENT</a>. In addition to reporting the score on the <em>overall</em> test set, we also evaluate each model on a more challenging subset consisting of out-of-domain examples. As the table below shows, the BERT-to-BERT model performs best in terms of both BLEU and PARENT. Moreover, all models achieve considerably lower performance on the challenge set indicating the challenge of out-of-domain generalization. </p><div style="width: 80%; margin: 0 auto;"><table align="center" cellpadding="0" cellspacing="5" style="margin-left: auto; margin-right: auto;">  <tbody>  <tr>   <td align="left">&nbsp;    </td>   <td align="center"><b>BLEU</b>   </td>   <td align="center"><b>PARENT</b>   </td>   <td align="center"><b>BLEU</b>   </td>   <td align="center"><b>PARENT</b>   </td>  </tr>  <tr>   <td align="left"><b>Model</b>   </td>   <td align="center"><b>(overall)</b>   </td>   <td align="center"><b>(overall)</b>   </td>   <td align="center"><b>(challenge)</b>   </td>   <td align="center"><b>(challenge)</b>   </td>  </tr>  <tr>   <td align="left">BERT-to-BERT     </td>   <td align="center">43.9    </td>   <td align="center">52.6    </td>   <td align="center">34.8    </td>   <td align="center">46.7    </td>  </tr>  <tr>   <td align="left">Pointer Generator    </td>   <td align="center">41.6    </td>   <td align="center">51.6    </td>   <td align="center">32.2    </td>   <td align="center">45.2    </td>  </tr>  <tr>   <td align="left">Puduppully et al. 2019    </td>   <td align="center">19.2    </td>   <td align="center">29.2    </td>   <td align="center">13.9    </td>   <td align="center">25.8    </td>  </tr>  </tbody></table></div><p>While automatic metrics can give some indication of performance, they are not currently sufficient for evaluating hallucination in text generation systems. To better understand hallucination, we manually evaluate the top performing baseline, to determine how faithful it is to the content in the source table, under the assumption that discrepancies indicate hallucination. To compute the “Expert” performance, for each example in our multi-reference test set, we held out one reference and asked annotators to compare it with the other references for faithfulness. As the results show, the top performing baseline appears to hallucinate information ~20% of the time.  </p><div style="width: 50%; margin: 0 auto;"><table align="center" cellpadding="0" cellspacing="5" style="margin-left: auto; margin-right: auto;">  <tbody>  <tr>   <td align="left"><b>&nbsp;</b>   </td>   <td align="center"><b>Faithfulness</b>   </td>   <td align="center"><b>Faithfulness</b></td>  </tr>  <tr>   <td align="left"><b>Model</b>   </td>   <td align="center"><b>(overall)</b>   </td>   <td align="center"><b>(challenge)</b></td>  </tr>  <tr>   <td align="left">Expert    </td>   <td align="center">93.6    </td>   <td align="center">91.4    </td>  </tr>  <tr>   <td align="left">BERT-to-BERT&nbsp;</td>   <td align="center">76.2    </td>   <td align="center">74.2    </td>  </tr>  </tbody></table></div><p><b>Model Errors and Challenges</b><br />In the table below, we present a selection of the observed model errors to highlight some of the more challenging aspects of the ToTTo dataset. We find that state-of-the-art models struggle with hallucination, numerical reasoning, and rare topics, even when using cleaned references (errors in red). The last example shows that even when the model output is correct it is sometimes not as informative as the original reference which contains more reasoning about the table (shown in blue). </p><div style="width: 90%; margin: 0 auto;"><table align="center" cellpadding="0" cellspacing="10" style="margin-left: auto; margin-right: auto;">  <tbody><tr>   <td><b>Reference</b>   </td>   <td><b>Model Prediction</b>   </td>  </tr>  <tr>   <td align="left"><em>in the 1939 currie cup, western province lost to transvaal by 17–6 in cape town.</em>   </td>    <td align="left"><em>the <font color="#ff0000"><b>first</b></font> currie cup was played in 1939 in <font color="#ff0000"><b>transvaal1</b></font> at new- lands, with western province <font color="#ff0000"><b>winning</b></font> 17–6.</em>   </td>  </tr>  <tr>   <td align="left"><em>a second generation of micro- drive was announced by ibm in 2000 with increased capacities at 512 mb and 1 gb.</em>   </td>    <td align="left"><em>there were <font color="#ff0000"><b>512 microdrive models in 2000</b></font>: 1 gigabyte.</em>   </td>  </tr>  <tr>   <td align="left"><em>the 1956 grand prix motorcy- cle racing season consisted of six grand prix races in five classes: 500cc, 350cc, 250cc, 125cc and sidecars 500cc.</em>   </td>   <td align="left"><em>the 1956 grand prix motorcycle racing season consisted of <font color="#ff0000"><b>eight</b></font> grand prix races in five classes: 500cc, 350cc, 250cc, 125cc and sidecars 500cc.</em>   </td>  </tr>  <tr>    <td align="left"><em>in travis kelce’s <font color="#337AFF"><b>last</b></font> collegiate season, he set personal <font color="#337AFF"><b>career highs</b></font> in receptions (45), re- ceiving yards (722), yards per receptions (16.0) and receiving touchdowns (8).</em>   </td>   <td align="left"><em>travis kelce finished the 2012 season with 45 receptions for 722 yards (16.0 avg.) and eight touchdowns.</em>   </td>  </tr>  </tbody></table></div><p><b>Conclusion</b><br />In this work, we presented ToTTo, a large, English table-to-text dataset that presents both a controlled generation task and a data annotation process based on iterative sentence revision. We also provided several state-of-the-art baselines, and demonstrated ToTTo could be a useful dataset for modeling research as well as for developing evaluation metrics that can better detect model improvements.  </p><p>In addition to the proposed task, we hope our dataset can also be helpful for other tasks such as <a href="https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html">table understanding</a> and sentence revision. ToTTo is available at our <a href="https://github.com/google-research-datasets/totto">GitHub repo</a>. </p><p><b>Acknowledgements</b><br /><em>The authors wish to thank Ming-Wei Chang, Jonathan H. Clark, Kenton Lee, and Jennimaria Palomaki for their insightful discussions and support. Many thanks also to Ashwin Kakarla and his team for help with the annotations. 	 		</em></p><div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=jeMkmAfQxOk:ZcargFg3tTk:yIl2AUoC8zA"><img border="0" src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" /></a>
</div><img alt="" height="1" src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/jeMkmAfQxOk" width="1" />

### [Adidas 炫酷大片「新年新愿，不牛不成」](https://app.vmovier.com/apiv3/post/view?postid=60988)

### [大年初一见！「唐人街探案3」终极预告](https://app.vmovier.com/apiv3/post/view?postid=60985)

### [燃爆全场！2D动画合辑「格斗」](https://app.vmovier.com/apiv3/post/view?postid=60978)

### [暖心乡村喜剧「三个老家伙」](https://app.vmovier.com/apiv3/post/view?postid=60977)

### [爷青回！霸气饭制短片「魔兽世界」](https://app.vmovier.com/apiv3/post/view?postid=60982)

### [还能这样？超有趣「乐高攀爬车的诞生」](https://app.vmovier.com/apiv3/post/view?postid=60962)

### [莫名疗愈，关于马男波杰克里的「戴安」](https://app.vmovier.com/apiv3/post/view?postid=60979)

### [百事&盆栽哥超级碗广告「准备好」](https://app.vmovier.com/apiv3/post/view?postid=60959)

### [编辑剧本必备技巧「电影的四种结局」](https://app.vmovier.com/apiv3/post/view?postid=60976)

### [Who I Am](http://feedproxy.google.com/~r/PoorlyDrawnLines/~3/Z2qHlW8LWyk/)

 <figure class="wp-block-image size-full is-resized"><a href="http://www.poorlydrawnlines.com/wp-content/uploads/2021/01/who_I_am.png"><img alt="" class="wp-image-7873" height="722" src="http://www.poorlydrawnlines.com/wp-content/uploads/2021/01/who_I_am.png" width="810" /></a></figure>
<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/PoorlyDrawnLines?a=Z2qHlW8LWyk:UKXDTPqr-nw:yIl2AUoC8zA"><img border="0" src="http://feeds.feedburner.com/~ff/PoorlyDrawnLines?d=yIl2AUoC8zA" /></a> <a href="http://feeds.feedburner.com/~ff/PoorlyDrawnLines?a=Z2qHlW8LWyk:UKXDTPqr-nw:qj6IDK7rITs"><img border="0" src="http://feeds.feedburner.com/~ff/PoorlyDrawnLines?d=qj6IDK7rITs" /></a> <a href="http://feeds.feedburner.com/~ff/PoorlyDrawnLines?a=Z2qHlW8LWyk:UKXDTPqr-nw:gIN9vFwOqvQ"><img border="0" src="http://feeds.feedburner.com/~ff/PoorlyDrawnLines?i=Z2qHlW8LWyk:UKXDTPqr-nw:gIN9vFwOqvQ" /></a>
</div><img alt="" height="1" src="http://feeds.feedburner.com/~r/PoorlyDrawnLines/~4/Z2qHlW8LWyk" width="1" />

### error read: https://rsshub.app/gov/ndrc/xwdt

### error read: http://www.bigdatainterview.com/feed/

### [ARM MacBook 14/16 新消息](https://www.v2ex.com/t/745295)

### error read: http://www.bigdatainterview.com/feed/

### [【Symmetrical Drawing Tool for Doodling】网页链接 用于涂鸦的对称绘图工具。 [图片]](https://weibo.com/1715118170/JDnFd8x3y)

### error read: http://www.bigdatainterview.com/feed/

### [[LG]《Asymmetric self-play for automatic goal discovery in robotic manipulation》O OpenAI, M Plappert, R Sampedro, T Xu, I Akkaya, V Kosaraju, P Welin...](https://weibo.com/1402400261/JDnTu2Xy3)

### [《今日学术视野(2021.1.16)》网页链接](https://weibo.com/1402400261/JDnN7hjGw)

### [早！[太阳] #早安# [图片]](https://weibo.com/1402400261/JDnJz9YNs)

### [【LT Browser - Next-gen browser to build, test & debug mobile websites】网页链接 LT浏览器-用于构建，测试和调试移动应用网站的下一代浏览器。对于想要创建...](https://weibo.com/1715118170/JDnQZdIFN)

### error read: https://rsshub.app/gov/ndrc/xwdt

### error read: http://www.bigdatainterview.com/feed/

### [小事 · 有些真相不必说](https://daily.zhihu.com/story/9732115)

### [马肉也是一种肉，为什么几乎没人吃？](https://daily.zhihu.com/story/9732050)

### [长期在不同脑力劳动中切换，对人的大脑有何影响？](https://daily.zhihu.com/story/9732047)

### [有哪些服装品牌擅长以黑白色为基调？](https://daily.zhihu.com/story/9732045)

### [为什么刘晓庆演的 95 版《武则天》的少女不违和？](https://daily.zhihu.com/story/9732040)

### [如何评价 2021 年 1 月新番《工作细胞 BLACK》？](https://daily.zhihu.com/story/9732035)

### [基于Prometheus的高可用Redis多实例监控实践](https://www.infoq.cn/article/feHAfD5XLK9TQ75F1pBU)

### [【How your website will be hacked if you have no CSRF protection】网页链接 如果没有CSRF保护，那么您的网站将如何被黑客入侵？ [图片]](https://weibo.com/1715118170/JDorN3ocv)

### [转发微博 - 转发 @网路冷眼:&ensp;#冷眼赠书福利# 联合 @博文视点Broadview 送出5本《技术人修炼之道：从程序员到百万高管的72项技能》。截止 1 月 22 日，转发...](https://weibo.com/1715118170/JDolrqnI6)

### [【Using Vue.js Alongside Django Template】网页链接 在Django模板旁使用Vue.js。网路冷眼技术分享 #科技暖心季# [图片]](https://weibo.com/1715118170/JDo3l8G6V)